{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import operator\n",
    "from __future__ import print_function\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    \"\"\"Reads a file into a list of phrases. Each phrase in the file must be separated with a new line character '\\n' \n",
    "    Args:\n",
    "        filepath (str): the relevant filepath of the file\n",
    "    Returns:\n",
    "        phrases (list(str)): A list with all the phrases \n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        phrases = f.readlines()\n",
    "    return phrases\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, trg_lm_file, tm_file, rm_file):\n",
    "        \n",
    "        self.trg_LM = self.parse_lm_file(trg_lm_file)\n",
    "        self.TM = self.parse_tm_file(tm_file)\n",
    "        self.RM = self.parse_rm_file(rm_file)\n",
    "        \n",
    "        self.distortion_limit = 3\n",
    "        \n",
    "    \n",
    "    def parse_lm_file(self, filepath):\n",
    "#         lm_file = load_file(filepath)\n",
    "\n",
    "#         LM = {}\n",
    "\n",
    "#         for line in lm_file:\n",
    "#             tokens = line.split()\n",
    "#             if len(tokens) > 0:\n",
    "#                 if is_number(tokens[0]):\n",
    "\n",
    "#                     p1 = float(tokens[0])\n",
    "\n",
    "#                     s = tokens[1:]\n",
    "\n",
    "#                     if is_number(s[-1]):\n",
    "#                         p2 = float(s[-1])\n",
    "#                         del s[-1]\n",
    "#                         LM[\" \".join(s)] = [p1,p2]\n",
    "#                     else:    \n",
    "#                         LM[\" \".join(s)] = [p1]\n",
    "\n",
    "#         return LM\n",
    "        lm_file = load_file(filepath)\n",
    "\n",
    "        LM = {}\n",
    "\n",
    "        for line in lm_file:\n",
    "            tokens = line.split('\\t')\n",
    "            if len(tokens) > 0:\n",
    "                \n",
    "                if is_number(tokens[0]):\n",
    "\n",
    "                    if(len(tokens) == 3):\n",
    "                        LM[tokens[1]] = [float(tokens[0]), float(tokens[2])]\n",
    "                    else:\n",
    "                        LM[tokens[1]] = [float(tokens[0])]\n",
    "\n",
    "        return LM\n",
    "\n",
    "\n",
    "    def parse_tm_file(self, filepath):\n",
    "        tm_file = load_file(filepath)\n",
    "\n",
    "        TM = {}\n",
    "\n",
    "        for line in tm_file:\n",
    "            tokens = line.split(\"|||\")\n",
    "\n",
    "            f = tokens[0].strip()\n",
    "            e = tokens[1].strip()\n",
    "\n",
    "            tokens = tokens[2].split()\n",
    "\n",
    "\n",
    "            p_fe = float(tokens[0])\n",
    "            l_fe = float(tokens[1])\n",
    "            p_ef = float(tokens[2])\n",
    "            l_ef = float(tokens[3])\n",
    "            wp = float(tokens[4])\n",
    "\n",
    "            TM[(f,e)] = [p_fe, l_fe, p_ef, l_ef, wp]\n",
    "\n",
    "        return TM\n",
    "\n",
    "\n",
    "    def parse_rm_file(self, filepath):\n",
    "        re_file = load_file(filepath)\n",
    "\n",
    "        reorderings = {}\n",
    "\n",
    "        for line in re_file:\n",
    "            tokens = line.split(\"|||\")\n",
    "\n",
    "            f = tokens[0].strip()\n",
    "            e = tokens[1].strip()\n",
    "\n",
    "            tokens = tokens[2].split()\n",
    "\n",
    "            m_rl = float(tokens[0])\n",
    "            s_rl = float(tokens[1])\n",
    "            d_rl = float(tokens[2])\n",
    "\n",
    "            m_lr = float(tokens[3])\n",
    "            s_lr = float(tokens[4])\n",
    "            d_lr = float(tokens[5])\n",
    "\n",
    "            reorderings[(f,e)] = [m_rl, s_rl, d_rl, m_lr, s_lr, d_lr]\n",
    "\n",
    "        return reorderings\n",
    "    \n",
    "    \n",
    "    def parse_trace(self, trace_line):\n",
    "        \n",
    "        tokens = trace_line.split(\"|||\")\n",
    "        \n",
    "        trace = []\n",
    "        for token in tokens: \n",
    "            [src_positions, translation] = token.split(':',1) \n",
    "            \n",
    "            src_positions = [int(pos) for pos in src_positions.split(\"-\")]\n",
    "            \n",
    "            trace.append([src_positions[0], src_positions[1], translation.strip()])\n",
    "        \n",
    "        return trace\n",
    "    \n",
    "    \n",
    "    def get_translation_cost(self, src_phrase, trace):\n",
    "        \n",
    "        trace = self.parse_trace(trace)\n",
    "        src_words = src_phrase.split()\n",
    "    \n",
    "        src_phrases = []\n",
    "        for p in trace:\n",
    "            src_phrase = \" \".join(src_words[p[0]:p[1] + 1])\n",
    "            src_phrases.append(src_phrase)\n",
    "            \n",
    "        \n",
    "        return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = load_file('data/file.test.de')\n",
    "trg_file = load_file('data/file.test.en') \n",
    "traces =  load_file('data/testresults.trans.txt.trace')\n",
    "\n",
    "decoder = Decoder('data/file.en.lm', 'data/phrase-table', 'data/dm_fe_0.75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -73.2276788907\n",
      "1 -110.043402691\n",
      "2 -191.224779675\n",
      "3 -109.614728338\n",
      "4 -151.9036692\n",
      "5 -184.737497751\n",
      "6 -74.7789894865\n",
      "7 -53.7341135257\n",
      "8 -57.8084059581\n",
      "9 -43.1085104777\n"
     ]
    }
   ],
   "source": [
    "def ngram_prob(ngram_words, LM):\n",
    "    \n",
    "    if not ngram_words:\n",
    "        return -4.0\n",
    "    \n",
    "    \n",
    "    ngram_str = \" \".join(ngram_words)\n",
    "    \n",
    "    if ngram_str in LM:\n",
    "        \n",
    "        return LM[ngram_str][0]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        backoff = 0\n",
    "        backoff_str = \" \".join(ngram_words[:-1])\n",
    "        if backoff_str in LM:\n",
    "            if len(LM[backoff_str]) == 2:\n",
    "                backoff = LM[backoff_str][1]\n",
    "            \n",
    "            \n",
    "        return ngram_prob(ngram_words[1:], LM) + backoff \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def get_lm_cost(phrase, hist, LM):\n",
    "    lm_cost = 0\n",
    "    \n",
    "    hist_words = hist.split()\n",
    "    phrase_words = phrase.split()\n",
    " \n",
    "    \n",
    "    for word in phrase_words:\n",
    "\n",
    "        lm_cost += ngram_prob(hist_words + [word], LM)\n",
    "        \n",
    "        if len(hist_words) == 4:\n",
    "            del hist_words[0]\n",
    "            \n",
    "        hist_words.append(word)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    return lm_cost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_translation_cost(src_phrase, trace, decoder):\n",
    "\n",
    "    MISS_COST = np.log10(0.001)\n",
    "    PHRASE_PENALTY = -1\n",
    "    HIST_SIZE = 4\n",
    "\n",
    "    total_cost = 0\n",
    "    \n",
    "    \n",
    "    trace = decoder.parse_trace(trace)\n",
    "    src_words = src_phrase.split()\n",
    "\n",
    "    src_phrases = [] \n",
    "    for p in trace:\n",
    "        src_phrase = \" \".join(src_words[p[0]:p[1] + 1])\n",
    "        src_phrases.append(src_phrase)  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(trace)):\n",
    "        \n",
    "        #-----------------------------TM COST-------------------------------------\n",
    "        tm_cost = 0\n",
    "        \n",
    "        pair = (src_phrases[i], trace[i][2])\n",
    "        \n",
    "        if pair in decoder.TM:   \n",
    "            [p_fe, lex_fe, p_ef, lex_ef, wp] = decoder.TM[pair]\n",
    "                \n",
    "            #h_TM = p(f|e) + p(e|f) (phrase translation)\n",
    "            tm_cost += 1.0 * (np.log10(p_fe) + np.log10(p_ef))\n",
    "                \n",
    "            #h_TM = l(f|e) + l(e|f) (lexical phrase translation)\n",
    "            tm_cost += 1.0 * (np.log10(lex_fe) + np.log10(lex_ef))\n",
    "\n",
    "            #h_wp (word penalty)\n",
    "            tm_cost += 1.0 * wp\n",
    "        else: \n",
    "            tm_cost += MISS_COST\n",
    "    \n",
    "        #---------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        \n",
    "        #-----------------------------LM COST-------------------------------------\n",
    "        \n",
    "        #Find the history of the target phrase, up to (n-1) words ('n' for maximum n-gram)\n",
    "        trg_hist = []\n",
    "        hist_count = 0\n",
    "        j = i-1\n",
    "        while j >= 0:\n",
    "            prev_trg_words = trace[j][2].split()\n",
    "            for trg_word in reversed(prev_trg_words):\n",
    "                trg_hist.insert(0, trg_word)\n",
    "                hist_count += 1\n",
    "                \n",
    "                if hist_count == HIST_SIZE:\n",
    "                    break;\n",
    "                \n",
    "            if hist_count == HIST_SIZE:\n",
    "                break;\n",
    "            j -= 1\n",
    "            \n",
    "        if hist_count < HIST_SIZE:\n",
    "            trg_hist.insert(0, '<s>')\n",
    "            \n",
    "        trg_hist = \" \".join(trg_hist)\n",
    "            \n",
    "        lm_cost = get_lm_cost(trace[i][2], trg_hist, decoder.trg_LM)\n",
    "        \n",
    "        #------------------------------------------------------------------------------\n",
    "        \n",
    "        #Distorion>\n",
    "        \n",
    "#         #Reordering pentaly\n",
    "        rm_cost = 0\n",
    "        if i > 0:\n",
    "            \n",
    "            if pair in decoder.RM:\n",
    "            \n",
    "                #continuous\n",
    "                if trace[i][0] == trace[i-1][1] + 1:\n",
    "                    rm_cost += decoder.RM[pair][3]\n",
    "\n",
    "                elif trace[i][1] == trace[i-1][0] - 1:\n",
    "                    rm_cost += decoder.RM[pair][4]\n",
    "\n",
    "                else:\n",
    "                    rm_cost += decoder.RM[pair][5]\n",
    "            \n",
    "            else:\n",
    "                rm_cost = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        total_cost += tm_cost + lm_cost + PHRASE_PENALTY\n",
    "    \n",
    "         \n",
    "    return total_cost\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    cost = get_translation_cost(src_file[i], traces[i], decoder)\n",
    "    print(i, cost)\n",
    "    print\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in traces:\n",
    "#     trace = decoder.parse_trace(line)\n",
    "    \n",
    "#     for pos in range(len(trace) - 1):\n",
    "        \n",
    "#         if trace[pos][1] + 1 != trace[pos + 1][0]:\n",
    "#             print(pos, trace)\n",
    "#             break\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.2063388"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lm_cost('same as', 'for junior doctors', decoder.trg_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'for junior doctors same'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-fe2b06568772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrg_LM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'for junior doctors same'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'for junior doctors same'"
     ]
    }
   ],
   "source": [
    "decoder.trg_LM['for junior doctors same']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'for junior doctors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-f50cbdb0ced6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# decoder.trg_LM['junior doctors same']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrg_LM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'for junior doctors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'for junior doctors'"
     ]
    }
   ],
   "source": [
    "# decoder.trg_LM['junior doctors same']\n",
    "decoder.trg_LM['for junior doctors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.06360954, -0.2467188]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoder.trg_LM['doctors same']\n",
    "decoder.trg_LM['junior doctors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4.204124, -0.288043]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.trg_LM['same']\n",
    "decoder.trg_LM['doctors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.8841028"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-4.349341 - 0.288043 - 0.2467188 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
