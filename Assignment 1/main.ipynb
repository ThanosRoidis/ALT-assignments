{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "import pickle\n",
    "import operator\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_file(filepath):\n",
    "    \"\"\"Reads a file into a list of phrases. Each phrase in the file must be separated with a new line character '\\n' \n",
    "    Args:\n",
    "        filepath (str): the relevant filepath of the file\n",
    "    Returns:\n",
    "        phrases (list(str)): A list with all the phrases \n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        phrases = f.readlines()\n",
    "    return phrases\n",
    "\n",
    "src_file = load_file('data/file.de')\n",
    "trg_file = load_file('data/file.en')\n",
    "aligned_file = load_file('data/file.aligned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract consistent pairs (f,e) along with their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(trg_start, trg_end, src_start, src_end, alignments, trg_aligned, trg_len):    \n",
    "    \"\"\" Checks if the subphrases of the source and target language are a consisnte pair, \n",
    "        assuming that no word in the source language is aligned with a word of the target language  outside the boundaries\n",
    "    Args:\n",
    "        trg_start (int): The target subphrase's first word position\n",
    "        trg_end (int): The target subphrase's last word position\n",
    "        src_start (int): The source subphrase's first word position\n",
    "        src_end (int): The source subphrase's last word position\n",
    "        alignments (list(tuple(int))): The alignment between the \n",
    "        trg_aligned (set(str)): A set with all the target language words that are aligned with at least one source word \n",
    "        trg_len (int): The number of words of the target language phrase\n",
    "        \n",
    "    Returns:\n",
    "        E (list(tuple(str))): A list with all of the extracted phrase pairs (f,e). This is the base (f,e) specified by the arguement and pairs that have a words appended on 'e' that are unaligned \n",
    "        A (list(list(tuple(int)))): A list with the alignment of the respective pair at E (A[i] has the alignments of E[i]) \n",
    "\n",
    "    \"\"\"\n",
    "    if trg_end < 0 :\n",
    "        return [], []\n",
    "    \n",
    "    A = []\n",
    "    \n",
    "    a = []\n",
    "    \n",
    "    for trg_word, src_word  in alignments:\n",
    "        if trg_start <= trg_word <= trg_end:\n",
    "            \n",
    "            if (src_word < src_start or src_word > src_end):  \n",
    "                return [], []\n",
    "            else:\n",
    "                \n",
    "                a.append( (trg_word - trg_start, src_word - src_start) )\n",
    "\n",
    "    E = []\n",
    "    trg_s = trg_start\n",
    "    \n",
    "    while True:\n",
    "        trg_e = trg_end\n",
    "        while True:\n",
    "            E.append( (\" \".join(trg_words[trg_s:trg_e+1]) , \" \".join(src_words[src_start:src_end+1]) ))\n",
    "            \n",
    "            A.append(a)\n",
    "             \n",
    "            trg_e += 1\n",
    "            \n",
    "            if trg_e in trg_aligned or trg_e == len(trg_words):\n",
    "                break \n",
    "\n",
    "        trg_s -= 1\n",
    "        \n",
    "        if trg_s in trg_aligned or trg_s < 0:\n",
    "            break\n",
    "            \n",
    "        # if an unaligned word was added at the beginning, increase the alignments positions by 1 \n",
    "       \n",
    "        a = [(i[0]+1,i[1]) for i in a]\n",
    "        \n",
    "    return E, A\n",
    "\n",
    "\n",
    "def extract_phrases(src_words, trg_words, alignments, cutoff):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        src_words (list(str)):\n",
    "        trg_words (list(str)):\n",
    "        alignments (list(tuple(int))):\n",
    "        cutoff (int): \n",
    "        \n",
    "    Returns:\n",
    "        extracted_phrases (list(tuple(str))):\n",
    "        extracted_alignments (list(tuple(int))):\n",
    "    \"\"\"\n",
    "    \n",
    "    if cutoff == -1:\n",
    "        cutoff = len(src_words)\n",
    "        \n",
    "        \n",
    "    trg_aligned = set()\n",
    "    for (trg,_) in alignments:\n",
    "        trg_aligned.add(trg)\n",
    "    \n",
    "    \n",
    "    extracted_phrases = []\n",
    "    extracted_alignments = []\n",
    "    # for every possible \n",
    "    for src_start in range(len(src_words)):\n",
    "        for src_end in range(src_start, min(src_start + cutoff, len(src_words))):\n",
    "\n",
    "            #Find the min and max position of the target words that the source phrase is aligned to\n",
    "            trg_start = len(trg_words) - 1\n",
    "            trg_end = - 1\n",
    "            for  (trg,src) in alignments: \n",
    "                if src_start <= src <= src_end:\n",
    "                    trg_start = min(trg, trg_start)\n",
    "                    trg_end = max(trg, trg_end)\n",
    "                    \n",
    "            \n",
    "            if(trg_end - trg_start > cutoff - 1):\n",
    "                continue\n",
    "\n",
    "            phrase_pairs, A = extract(trg_start, trg_end, src_start, src_end, alignments, trg_aligned, len(trg_words))\n",
    "            \n",
    "            if (phrase_pairs):\n",
    "                extracted_phrases.extend(phrase_pairs)\n",
    "                for a in A:\n",
    "                    extracted_alignments.append(a);\n",
    "            \n",
    "    return extracted_phrases, extracted_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49501 / 50000 (56s)                                                                                               \n",
      "Done!!!\n",
      "Total duration: 56s\n",
      "#unique en phrases: 1363321\n",
      "#unique de phrases: 1264713\n",
      "#unique (en,de) phrases: 2691953\n"
     ]
    }
   ],
   "source": [
    "def add_freq_dict(dictionary, item):\n",
    "    \"\"\"Increases the frequency of an phrase/item in the dictionary by 1, or adds it with a frequency of 1 if it doesn't exist yet\n",
    "    Args:\n",
    "        dictionary (dict): a dictionary that holds the frequency of an a phrase (item)\n",
    "        item (str): a string whose frequency will be increased\n",
    "    \"\"\"\n",
    "    if item in dictionary:\n",
    "        dictionary[item] += 1\n",
    "    else:\n",
    "        dictionary[item] = 1\n",
    "\n",
    "        \n",
    "# A dictionary that maps a source phrase f to its frequency freq(f)\n",
    "freq_src = dict() #\n",
    "\n",
    "# A dictionary that maps a target phrase e to its frequency freq(e)\n",
    "freq_trg = dict() #\n",
    "\n",
    "# A dictionary f(e,f)->[freq(e,f), A] which initially maps a consistent pair (e,f) to its frequency freq(e,f) and a set A with all existing alignments between them.\n",
    "# Additional information is added consequently to the value list, like p(f|e), p(e|f), l(f|e) and l(e|f)\n",
    "freq_pairs = dict() \n",
    "\n",
    "start_time = time.time()\n",
    "    \n",
    "for i in range(len(src_file)):\n",
    "    src_words = src_file[i].split()\n",
    "    trg_words = trg_file[i].split()        \n",
    "    alignments = [[int(a) for a in alignment.split('-')] for alignment in aligned_file[i].split()] #convert to list of int pairs\n",
    "    alignments = [(al[1], al[0]) for al in alignments] #reverse their order so that they are (trg,src) instead of (src,trg)\n",
    "    \n",
    "    #Extract all the consistent phrase pairs with their alignments\n",
    "    phrase_pairs, extracted_alignments = extract_phrases(src_words, trg_words, alignments, 5)\n",
    "     \n",
    "    #Add the phrases to the dictionaries\n",
    "    for j, pair in enumerate(phrase_pairs):\n",
    "        \n",
    "        a = extracted_alignments[j]\n",
    "       \n",
    "        # If the pair (f,e) already exists, then increase its frequency and add the new alignment\n",
    "        if pair in freq_pairs:\n",
    "            freq_pairs[pair][0] += 1;\n",
    "            freq_pairs[pair][1].add(tuple(a));\n",
    "        else:\n",
    "             freq_pairs[pair] = [1, set([tuple(a)])]\n",
    "        \n",
    "        add_freq_dict(freq_trg, pair[0]) #increase the frequency of the source phrase f\n",
    "        add_freq_dict(freq_src, pair[1]) #increase the frequency of the target phrase e\n",
    "        \n",
    "    #show real time progress\n",
    "    if (i % (len(src_file) / 100) == 0):\n",
    "        print(' \\r%d / %d (%ds)'%(i+1,len(src_file), time.time() - start_time), end = '')\n",
    "\n",
    "print()\n",
    "print('Done!!!')\n",
    "print('Total duration: %ds'%(time.time() - start_time))\n",
    "\n",
    "print(\"#unique en phrases: %d\"%(len(freq_trg)))\n",
    "print(\"#unique de phrases: %d\"%(len(freq_src)))\n",
    "print(\"#unique (en,de) phrases: %d\"%(len(freq_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate p(e|f) and p(f|e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculates the probabilities p(e|f) and p(f|e) for every (f,e) pair in freq_pairs and appends it to the end of freq_pairs(f,e)\n",
    "for trg_src_pair, value in freq_pairs.iteritems():\n",
    "    freq = value[0]\n",
    "    \n",
    "    p_src_trg = 1.0 * freq / freq_trg[trg_src_pair[0]]\n",
    "    p_trg_src = 1.0 * freq / freq_src[trg_src_pair[1]]\n",
    "    \n",
    "    freq_pairs[trg_src_pair] = [freq, value[1], p_src_trg, p_trg_src]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate lexical translation probabilities (KMO approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2691900 / 2691953 (99%) (216s)                                                                                              \n",
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "def KMO(src_words, trg_words, possible_alignments, w_trg_src):\n",
    "    \"\"\" Returns the lexical weight l(e|f) between a phrase pair (e,f) using the KMO approach\n",
    "    Args:\n",
    "        src_words (list(str)): the words of the f phrase\n",
    "        trg_words (list(str)): the words of the e phrase\n",
    "        possible_alignments (list(list(tuple(int)): all of the possible alignments 'a' between these 2 phrases\n",
    "        w_trg_src (dict(tuple(str))): A dictionary with the word translation probabilities\n",
    "    Return:\n",
    "        lex (float): l(e|f)\n",
    "    \"\"\"\n",
    "    lex = -1\n",
    "    \n",
    "    for a in possible_alignments:\n",
    "    \n",
    "        #each position holds for a word 'e' all of the w(e|f) with its aligned words 'f'\n",
    "        mat = len(trg_words) * [None] \n",
    "\n",
    "        for (trg_pos, src_pos) in a:\n",
    "\n",
    "            src_word = src_words[src_pos]\n",
    "            trg_word = trg_words[trg_pos]\n",
    "\n",
    "            if mat[trg_pos] is None:\n",
    "                mat[trg_pos] = []\n",
    "\n",
    "            \n",
    "            # if these two words are paired in the word translation probabilities map, then take the probability\n",
    "            \n",
    "            if (trg_word, src_word) in w_trg_src:\n",
    "                w = w_trg_src[(trg_word, src_word)] # w(e|f)\n",
    "                mat[trg_pos].append(w)\n",
    "                \n",
    "            # else it is zero\n",
    "            else:\n",
    "                mat[trg_pos].append(0)\n",
    "\n",
    "        #Calculate the mean for each 'e' word\n",
    "        for i in range(len(mat)):\n",
    "            # if the e word was unaligned, then it is 1, basically ignoring it\n",
    "            if mat[i] is None:\n",
    "                mat[i] = 1\n",
    "            else:\n",
    "                mat[i] = numpy.mean(mat[i])\n",
    " \n",
    "    \n",
    "        lex = max(lex, numpy.product(mat))\n",
    "    \n",
    "    return lex;\n",
    "\n",
    "\n",
    "c = 1\n",
    "start_time = time.time()\n",
    "\n",
    "for trg_src_pair, value in freq_pairs.iteritems():\n",
    "    \n",
    "    #calculate p(e|f)\n",
    "    trg_words = trg_src_pair[0].split()\n",
    "    src_words = trg_src_pair[1].split()\n",
    "    possible_alignments = value[1]\n",
    "    w_trg_src = {}''\n",
    "    for a in possible_alignments:\n",
    "        for (trg_pos, src_pos) in a:\n",
    "            pair = (trg_words[trg_pos], src_words[src_pos])\n",
    "            if pair in freq_pairs:\n",
    "                w_trg_src[pair] = freq_pairs[pair][3]\n",
    "                \n",
    "                \n",
    "    lex_trg_src = KMO(src_words, trg_words, possible_alignments, w_trg_src);\n",
    "    \n",
    "    #calcuate p(f|e) - Reverse the source and target language\n",
    "    trg_words , src_words = src_words , trg_words\n",
    "\n",
    "    #reverse the alignments from (trg,src) to (src,trg)\n",
    "    possible_alignments2 = set()\n",
    "    for a in possible_alignments:\n",
    "        a2 = tuple([(al[1], al[0]) for al in a])\n",
    "        possible_alignments2.add(a2)\n",
    "        \n",
    "    w_src_trg = {}\n",
    "    for a in possible_alignments2:\n",
    "        for (trg_pos, src_pos) in a:\n",
    "            inv_pair = (src_words[src_pos], trg_words[trg_pos])\n",
    "            if inv_pair in freq_pairs:\n",
    "                w_src_trg[(inv_pair[1], inv_pair[0])] = freq_pairs[inv_pair][2]\n",
    "    \n",
    "    lex_src_trg = KMO(src_words, trg_words, possible_alignments2, w_src_trg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    freq_pairs[trg_src_pair].append(lex_src_trg)\n",
    "    freq_pairs[trg_src_pair].append(lex_trg_src)\n",
    "        \n",
    "    if  c % (len(freq_pairs) / 100) == 0: \n",
    "        print(' \\r%d / %d (%d%%) (%ds)'%(c,len(freq_pairs), 100 * c / len(freq_pairs),time.time() - start_time), end = '')\n",
    "    c +=1\n",
    "        \n",
    "\n",
    "\n",
    "print('\\nDone!!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sorted_pairs = sorted(freq_pairs.keys(), key=operator.itemgetter(1), reverse = False)\n",
    "output_folder = 'C:/Users/Thanos/Desktop/outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the first file containg the frequencies:   *f ||| e ||| freq(f) freq(e) freq(f; e)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2691900 / 2691953 (99%) (10s)                                       \n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "start_time = time.time();\n",
    "with open(output_folder + 'output1.txt', 'w') as the_file:\n",
    "\n",
    "    for pair in sorted_pairs: \n",
    "        f = pair[1]\n",
    "        e = pair[0]\n",
    " \n",
    "        freq_f = freq_src[f]\n",
    "        freq_e = freq_trg[e]\n",
    "        freq_fe = freq_pairs[pair][0]\n",
    "        \n",
    "        the_file.write('%s ||| %s ||| %d %d %d\\n' % (f,e,freq_f, freq_e, freq_fe))\n",
    "\n",
    "        if  c % (len(freq_pairs) / 100) == 0: \n",
    "            print(' \\r%d / %d (%d%%) (%ds)'%(c,len(sorted_pairs), 100 * c / len(sorted_pairs),time.time() - start_time), end = '')\n",
    "        c +=1\n",
    "\n",
    "print(\"\\nDone!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the second file containing the probabilities:  * f ||| e ||| p(f|e) p(e|f)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2691900 / 2691953 (99%) (8s)                               \n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "start_time = time.time();\n",
    "with open(output_folder + 'output2.txt', 'w') as the_file:\n",
    "\n",
    "    for pair in sorted_pairs: \n",
    "        f = pair[1]\n",
    "        e = pair[0]\n",
    " \n",
    "        p_f_e = freq_pairs[pair][2]\n",
    "        p_e_f = freq_pairs[pair][3]\n",
    "        \n",
    "        the_file.write('%s ||| %s ||| %f %f\\n' % (f, e, p_f_e, p_e_f))\n",
    "\n",
    "        if  c % (len(freq_pairs) / 100) == 0: \n",
    "            print(' \\r%d / %d (%d%%) (%ds)'%(c,len(sorted_pairs), 100 * c / len(sorted_pairs),time.time() - start_time), end = '')\n",
    "        c +=1\n",
    "\n",
    "print(\"\\nDone!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the third file containing the probabilities and the lexical weights:* f ||| e ||| p(f|e) p(e|f) l(f|e) l(e|f)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2691900 / 2691953 (99%) (11s)                                         \n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "start_time = time.time();\n",
    "with open(output_folder + 'output3.txt', 'w') as the_file:\n",
    "\n",
    "    for pair in sorted_pairs: \n",
    "        f = pair[1]\n",
    "        e = pair[0]\n",
    " \n",
    "        p_f_e = freq_pairs[pair][2]\n",
    "        p_e_f = freq_pairs[pair][3]\n",
    "        \n",
    "        l_f_e = freq_pairs[pair][4]\n",
    "        l_e_f = freq_pairs[pair][5]\n",
    "        \n",
    "        \n",
    "        the_file.write('%s ||| %s ||| %f %f %f %f\\n' % (f, e, p_f_e, p_e_f, l_f_e, l_e_f))\n",
    "\n",
    "        if  c % (len(freq_pairs) / 100) == 0: \n",
    "            print(' \\r%d / %d (%d%%) (%ds)'%(c,len(sorted_pairs), 100 * c / len(sorted_pairs),time.time() - start_time), end = '')\n",
    "        c +=1\n",
    "\n",
    "print(\"\\nDone!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the fourth file containing everything:* f ||| e ||| p(f|e) p(e|f) l(f|e) l(e|f) |||  freq(f) freq(e) freq(f; e)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2691900 / 2691953 (99%) (16s)                                              \n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "start_time = time.time();\n",
    "\n",
    "with open(output_folder + 'output_all.txt', 'w') as the_file:\n",
    "    for pair in sorted_pairs: \n",
    "        f = pair[1]\n",
    "        e = pair[0]\n",
    "        \n",
    "        freq_f = freq_src[f]\n",
    "        freq_e = freq_trg[e]\n",
    "        freq_fe = freq_pairs[pair][0]\n",
    " \n",
    "        p_f_e = freq_pairs[pair][2]\n",
    "        p_e_f = freq_pairs[pair][3]\n",
    "        \n",
    "        l_f_e = freq_pairs[pair][4]\n",
    "        l_e_f = freq_pairs[pair][5]\n",
    "        \n",
    "        \n",
    "        the_file.write('%s ||| %s ||| %f %f %f %f ||| %d %d %d \\n' % (f, e, p_f_e, p_e_f, l_f_e, l_e_f, freq_f, freq_e, freq_fe))\n",
    "\n",
    "        if  c % (len(freq_pairs) / 100) == 0: \n",
    "            print(' \\r%d / %d (%d%%) (%ds)'%(c,len(sorted_pairs), 100 * c / len(sorted_pairs),time.time() - start_time), end = '')\n",
    "        c +=1\n",
    "\n",
    "print(\"\\nDone!!!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
